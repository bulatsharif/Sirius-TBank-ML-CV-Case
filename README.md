# Sirius x TBank Computer Vision Case

Этот репозиторий содержит решение кейса для отбора в смену по ML от Т‑Банка и Сириуса.

## Подробное описание подхода к решению

Прежде чем приступить к решению, я наметил стратегию:
1. Предобработать датасет и выбрать способ разметки данных.
2. Сделать базовый сервис для детекции логотипов на основе выбранного метода разметки.
3. На размеченных данных провести fine‑tune YOLOv11 и интегрировать модель в сервис.

Первым делом я просмотрел датасет: он состоял из обычных изображений с большим количеством логотипов различных компаний.

Далее я начал изучать, как автоматически размечать датасет для задачи детекции.

В процессе поиска я наткнулся на подход SIFT + RANSAC. Этот метод также хорош тем, что избавляет от множества проблем: он устойчив к ротациям, различным масштабам и изменениям цвета. К тому же для него есть готовый туториал от библиотеки [OpenCV](https://docs.opencv.org/4.x/d1/de0/tutorial_py_feature_homography.html). Протестировав несколько изображений, я решил остановиться на этом методе.

Я реализовал функции и подготовил ноутбук, который можно запустить на Kaggle для аннотации датасета. На каждом изображении я дополнительно проверял наличие логотипа «Тинькофф» или текста «Тинькофф» и, если они присутствовали, не возвращал боксы логотипа. Эту версию ноутбука я отправил на выполнение в Kaggle.

Пока ноутбук выполнялся на Kaggle, я создал базовый сервис на FastAPI для получения предсказаний по изображению. Для детекции логотипа использовался тот же метод, что и для аннотации. Также я разработал простой интерфейс на основе Streamlit.

Однако я получил неприемлемо низкий recall — изображения даже с явным логотипом не аннотировались. С полученным размеченным датасетом я попытался обучить YOLO (yolo11m), однако модель практически не обучилась, поэтому я начал улучшать датасет.

![Результаты первой YOLO](./training_results/results_first_yolo.png)

Я начал повышать recall. Во‑первых, убрал проверку на логотипы «Тинькофф». Полагаю, логотип «Тинькофф» содержит стилизованную «Т», похожую на «Т» в логотипе Т‑Банка, из‑за чего даже корректные логотипы Т‑Банка отклонялись. Во‑вторых, долгое время у меня оставались параметры из туториала по OpenCV, в котором решалась немного другая задача. Поэтому я начал настраивать их под свою задачу, чтобы повысить recall.

Визуально метод стал намного чаще детектировать логотип, но начал находить и шум. Этот шум легко отфильтровывался: его бокс обычно превращался в линию. Следовательно, я добавил проверку на площадь бокса перед возвратом координат. Так я повысил recall, убрав значительную часть шума.

С обновлённым методом я обработал на Kaggle 2000 изображений из датасета. Было заметно, что метод довольно хорошо аннотирует данные, хотя иногда принимал фон или шум за логотип Т‑Банка. На полученной разметке я обучил YOLO. Параметры обучения были такими:

```python
results = model.train(
    data='/kaggle/working/data.yaml',
    epochs=20,
    batch=64,
    device=[0, 1],
    freeze=8,
    lr0=3e-3,
    augment=True,
    lrf=0.1
)
```

Эта модель уже добилась намного лучших результатов.

![Результаты второй YOLO](./training_results/results_second_yolo.png)

Метрики кажутся невысокими, однако они занижены, поскольку разметка датасета всё ещё была некачественной.

Можно посмотреть на пример одного батча:

Разметка:
![Разметка](./training_results/some_annotated.jpg)

Предсказания:
![Предикт YOLO](./training_results/some_predicted.jpg)

Видно, что YOLO не предсказывала шум.

Чтобы качественно протестировать модель, я подготовил тестовый набор: нашёл в Google различные изображения, связанные с Т‑Банком, и несвязанные изображения. Затем разметил их с помощью X‑AnyLabeling. Набор доступен в папке `test_set`.

Ниже — метрики на валидационном сете и примеры детекции.

Kaggle‑ноутбуки (все версии):

1. [Автоматическая разметка (SIFT + RANSAC)](https://www.kaggle.com/code/bulatsharipov1/sirius-tbank-annotate-dataset/notebook)
2. [Обучение YOLO](https://www.kaggle.com/code/bulatsharipov1/sirius-tbank-train-yolo)

## Архитектура (бэкенд и веб‑приложение)

### Бэкенд (FastAPI)
- Основной вход: `app/main.py` — создаёт приложение FastAPI, через lifespan прогревает YOLO‑модель, подключает роутер `predict`, настраивает обработчики ошибок, эндпоинты `/health` и `/`.
- Роутер: `app/api/routers/predict.py` — `POST /api/v1/detect` принимает файл изображения, валидирует `Content-Type`, декодирует через OpenCV, вызывает `detect_logo_YOLO`, возвращает список детекций с абсолютными координатами `xyxy`.
- Схемы: `app/schemas/predict.py` — модели `BoundingBox`, `Detection`, `DetectionResponse`, `ErrorResponse` (валидация и контракт ответа/ошибок).
- Модель: `app/models/detect_YOLO.py` — ленивое кэширование `YOLO` из `app/core/config.YOLO_MODEL_PATH`, предсказание возвращает боксы `xyxy`, корректная обработка ошибок загрузки.
- Конфигурация: `app/core/config.py` — константы и пути (`YOLO_MODEL_PATH`, `ALLOWED_IMAGE_CONTENT_TYPES`).

Поток запроса: загрузка файла → проверка типа/пустоты → декодирование в BGR → инференс YOLO → приведение координат к int и формирование `DetectionResponse` → JSON‑ответ (пустой список, если ничего не найдено).

### Веб‑приложение (Streamlit)
- Файл: `streamlit_app.py` — простой UI: загрузка изображения, локальный вызов `detect_logo_YOLO` (без REST), отрисовка боксов и времени инференса. Может запускаться отдельно от API.

### Контейнеризация и запуск
- `Dockerfile` — многоэтапная сборка, изолированное venv, системные библиотеки для OpenCV, запуск через `gunicorn` + `uvicorn.workers.UvicornWorker` от непривилегированного пользователя.
- `compose.yaml` — сервис `app`, публикация порта `8000`, healthcheck по `/health`.
- `compose.dev.yaml` — режим разработки: `--reload` и монтирование `./app` в контейнер.

## Инструкции по запуску и использованию

Есть несколько способов запустить проект:

1. Docker Compose (обычный запуск)

```bash
docker compose up --build
```

2. Для разработки (автоматический reload)
```bash
docker compose -f compose.dev.yaml up --build
```

3. Запуск Streamlit для веб‑интерфейса
```bash
pip install -r requirements.txt
streamlit run streamlit_app.py
```

### Оценка на тестовом сете (CLI)

Для удобной оценки модели, я сделал CLI скрипт, который можно запустить чтобы прогнать YOLO через тестовый датасет. На выходе получим метрики

```bash
python scripts/evaluate.py 
```

Также есть настраиваемые параметры:
- `--weights`: путь к весам модели
- `--data`: путь к тестовому датасету (должен быть в формате YOLO)
- `--device`: Запуск на GPU или CPU

По дефолту запустит на используемой модели `app/models/yolo_model.pt`, и с тестовым датасетом по пути - `test_set`

Пример вывода скрипта:
```
=== Validation Metrics ===
Precision@0.5: 0.932685
Recall@0.5:    0.842105
F1@0.5:        0.885000
mAP@0.5:       0.882671
mAP@0.5:0.95:  0.519325

=== Speed (per image) ===
preprocess: 1.0 ms
inference:  110.0 ms
postprocess: 1.6 ms
```

## Результаты работы модели на валидационной выборке

Ниже приведены метрики модели на валидационном сете (больше результатов — в папках `notebooks` и `notebooks/runs`):
```
metrics/precision(B): 0.9326854956255276
metrics/recall(B): 0.8421052631578947
F1@0.5(B): 0.8850
metrics/mAP50(B): 0.8826712429727135
metrics/mAP50-95(B): 0.519325435244222
fitness: 0.519325435244222
```

Пример предсказаний:
![Пример предиктов финальной YOLO](/notebooks/runs/detect/val4/val_batch0_pred.jpg)

## Дополнительно — комментарии к решению

### Альтернативные подходы к решению

Некоторые идеи для автоматической аннотации, над которыми я думал:
- Использовать SAM, чтобы сегментировать объекты на изображении, а затем проверять каждый сегмент на сходство с логотипом Т‑Банка.
    - Минусы: SAM, скорее всего, будет сегментировать нерелевантные области (например, может выделить букву «T» в логотипе); проверка всех сегментов на сходство может занимать слишком много времени; неочевидно, как надёжно измерять сходство между двумя изображениями; нужно отсеивать случаи, где логотипа нет, то есть подбирать порог; наконец, SAM требует значительного времени на инференс (даже FastSAM и MobileSAM работали неприемлемо медленно), поэтому обработка всего датасета заняла бы слишком много времени.
- Использовать template matching: «прокатывать» шаблон логотипа по изображению и измерять сходство.
    - Минусы: метод не учитывает ротации, разные масштабы и изменения цвета; неясно, как надёжно вычислять сходство, и, вероятно, потребуется подбирать порог; по опыту, точность у такого подхода невысокая.
- Разметить самостоятельно достаточно изображений, чтобы дообучить YOLO, затем итеративно использовать модель для улучшения датасета и переобучать её.
    - Минусы: подход выглядит наиболее адекватным, однако в рамках задания хотелось опробовать методы автоматической разметки.

### Анализ текущих проблем решения и возможные улучшения

- В работе использовано только подмножество изображений из датасета. При наличии большего количества времени можно обработать весь исходный набор и обучить YOLO на нём. Это, вероятно, дало бы лучшие результаты.
- Модель YOLO всё ещё может пропускать логотипы либо быть не слишком уверенной в ответах. Можно итеративно улучшать датасет: размечать с помощью YOLO, дополнять другими методами и переобучать модель.
- Использованы данные только из готового источника. Можно спарсить и добавить больше данных. Также можно генерировать синтетические данные: размещать логотип Т‑Банка где‑нибудь на изображении, применять различные трансформации и таким образом обогащать набор данных.
- SIFT + RANSAC всё ещё плохо аннотировал датасет, хотя и обеспечивал более высокий recall. Можно дополнительно поиграть с параметрами или доработать метод, чтобы повышать качество разметки.

### Анализ производительности решения и варианты ускорения

В текущем виде решение достаточно эффективно: на MacBook M3 Pro обработка одного изображения занимает ≈ 0,1 секунды. Модель довольно легковесная, имеет 20 миллионов параметров и весит всего 40мб. Модель находится по пути: ```app/models/yolo_model.pt```.

Возможные оптимизации:
- Перевести модель в ONNX и использовать оптимизации для инференса.
- Задача относительно простая: можно попробовать меньшие модели YOLO (например, `yolo11n`, `yolo11s`).

*Выполнил: Шарипов Булат*
